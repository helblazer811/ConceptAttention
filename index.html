<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="alechelbling.com/ConceptAttention"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/teaser.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>ConceptAttention: Diffusion Transformers Learn Highly Interpretable Features </title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">ConceptAttention: Diffusion Transformers Learn Highly Interpretable Features</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="alechelbling.com" target="_blank">Alec Helbling<sup> 1</sup></a>,</span>
                <span class="author-block">
                  <a href="https://tunahansalih.github.io/" target="_blank">Tuna Han Salih Meral<sup> 2</sup></a>,</span>
                  <span class="author-block">
                    <a href="https://bhoov.com/" target="_blank">Ben Hoover<sup> 1, 3</sup></a>,</span>
                  </span>
                  <span class="author-block">
                    <a href="https://pinguar.org/" target="_blank">Pinar Yanardag<sup> 2</sup></a>,</span>
                  </span>
                  <span class="author-block">
                    <a href="https://poloclub.github.io/polochau/" target="_blank">Duen Horng (Polo) Chau<sup> 1</sup></a>
                  </span>
                  </div>
                 
                  <div class="is-size-5 publication-authors">
                    <span class="author-block">Georgia Tech <sup> 1 </sup>, Virginia Tech<sup> 2 </sup>, IBM Research<sup> 3 </sup></span>
                  </div>

                  <div class="column has-text-centered">
                    <!-- Arxiv PDF link -->
                    <!-- <div class="publication-links">
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/<ARXIV PAPER ID>.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span> -->

                    <!-- Supplementary PDF link -->
                    <!-- <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a> -->
                  <!-- </span> -->
                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/helblazer811/ConceptAttention" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                        <i class="fab fa-github"></i>
                        </span>
                        <span>Code</span>
                    </a>
                </span>
                <span class="link-block">
                  <a href="https://huggingface.co/spaces/helblazer811/ConceptAttention" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <!-- <span class="icon"> -->
                      <!-- <i class="fab fa-github"></i> -->
                      <!-- </span> -->
                      <span>Demo</span>
                  </a>
              </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                    <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                    <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <!-- <video poster="" id="tree" autoplay controls muted loop height="100%">
        <source src="static/videos/banner_video.mp4"
        type="video/mp4">
      </video> -->
      <img src="static/images/WebsiteTeaser.png" alt="MY ALT TEXT"/>
      <h2 class="subtitle has-text-centered">
        ConceptAttention produces saliency maps that precisely localize the presence of textual concepts in images. 
    </h2>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Do the rich representations of multi-modal diffusion transformers (DiTs) exhibit unique properties that enhance their interpretability? 
            We introduce ConceptAttention, a novel method that leverages the expressive power of DiT attention layers to generate high-quality saliency 
            maps that precisely locate textual concepts within images. Without requiring additional training, ConceptAttention repurposes the parameters 
            of DiT attention layers to produce highly contextualized concept embeddings, contributing the major discovery that performing linear 
            projections in the output space of DiT attention layers yields significantly sharper saliency maps compared to commonly used 
            cross-attention mechanisms. Remarkably, ConceptAttention even achieves state-of-the-art performance on zero-shot image segmentation 
            benchmarks, outperforming 11 other zero-shot interpretability methods on the ImageNet-Segmentation dataset and on a single-class 
            subset of PascalVOC. Our work contributes the first evidence that the representations of multi-modal DiT models like Flux are 
            highly transferable to vision tasks like segmentation, even outperforming multi-modal foundation models like CLIP. 
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/ConceptAttentionExamples.png" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
            ConceptAttention can generate high-quality saliency maps for multiple concepts simultaneously.
            Additionally, our approach is not restricted to concepts in the prompt vocabulary. 
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/ThreeRowSegmentation.png" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
            ConceptAttention produces higher fidelity raw scores and saliency maps than alternative methods, sometimes surpassing in quality even the ground truth saliency map provided by the ImageNet-Segmentation task. Top row shows the soft predictions of each method and the bottom shows the binarized predictions. 
        </h2>
      </div>
  </div>
</div>
</div>
</section>
<!-- End image carousel -->



<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>BibTex Code Here</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
